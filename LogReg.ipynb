{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSX0E8XEdlE6"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uHoY8iSWdocP"
   },
   "outputs": [],
   "source": [
    "#------------ No New Package --------------\n",
    "import numpy as np\n",
    "#---------------------------------------------------------\n",
    "\n",
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier using Stochastic Gradient Descent (SGD)\n",
    "    for binary classification problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the Logistic Regression model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            The step size for updating model parameters\n",
    "        n_epochs : int\n",
    "            Number of passes through the training data\n",
    "        batch_size : int\n",
    "            Number of training examples to use in each gradient update\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.w = None  # weights\n",
    "        self.b = None  # bias\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "        \"\"\"\n",
    "        Compute the sigmoid activation function.\n",
    "\n",
    "        Formula:\n",
    "        σ(z) = 1 / (1 + e^(-z))\n",
    "        where z = wx + b\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        z : array-like\n",
    "            Input values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like\n",
    "            Sigmoid of input values\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def initialize_parameters(self, n_features):\n",
    "        self.w = np.random.randn(n_features) * np.sqrt(2 / n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize model parameters using Xavier initialization.\n",
    "\n",
    "        Formula for Xavier initialization:\n",
    "        w ~ N(0, sqrt(2/n_features))\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_features : int\n",
    "            Number of input features\n",
    "        \"\"\"\n",
    "        # Xavier initialization for better convergence\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "\n",
    "        Formula:\n",
    "        L = -1/N * Σ(y * log(ŷ) + (1-y) * log(1-ŷ))\n",
    "        where:\n",
    "        - y is true label\n",
    "        - ŷ is predicted probability\n",
    "        - N is number of samples\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like\n",
    "            True binary labels\n",
    "        y_pred : array-like\n",
    "            Predicted probabilities\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average binary cross-entropy loss\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def compute_gradients(self, X_batch, y_batch, y_pred):\n",
    "        m = X_batch.shape[0]\n",
    "        dw = np.dot(X_batch.T, (y_pred - y_batch)) / m\n",
    "        db = np.sum(y_pred - y_batch) / m\n",
    "        return dw, db\n",
    "        \"\"\"\n",
    "        Compute gradients for weights and bias.\n",
    "\n",
    "        Formulas:\n",
    "        ∂L/∂w = 1/N * X^T * (ŷ - y)\n",
    "        ∂L/∂b = 1/N * Σ(ŷ - y)\n",
    "        where:\n",
    "        - X is input features\n",
    "        - y is true labels\n",
    "        - ŷ is predicted probabilities\n",
    "        - N is batch size\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_batch : array-like\n",
    "            Input features for current batch\n",
    "        y_batch : array-like\n",
    "            True labels for current batch\n",
    "        y_pred : array-like\n",
    "            Predicted probabilities for current batch\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Gradients for weights and bias\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.initialize_parameters(n_features)\n",
    "\n",
    "        \"\"\"\n",
    "        Train the logistic regression model using mini-batch SGD.\n",
    "\n",
    "        Process:\n",
    "        1. Initialize parameters\n",
    "        2. For each epoch:\n",
    "            a. Shuffle data\n",
    "            b. Split into mini-batches\n",
    "            c. For each mini-batch:\n",
    "                - Compute forward pass (sigmoid)\n",
    "                - Compute gradients\n",
    "                - Update parameters\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        # Initialize Parameters\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "        # Epoch Loop\n",
    "        for epoch in range(self.n_epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            # Shuffle the data\n",
    "            # ===== Insert your code here =====\n",
    "\n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "    \n",
    "                y_pred = self.sigmoid(np.dot(X_batch, self.w) + self.b)\n",
    "                dw, db = self.compute_gradients(X_batch, y_batch, y_pred)\n",
    "    \n",
    "                self.w -= self.learning_rate * dw\n",
    "                self.b -= self.learning_rate * db\n",
    "                # Get Batch Data\n",
    "                # ===== Insert your code here =====\n",
    "\n",
    "                # Forward pass\n",
    "                # ===== Insert your code here =====\n",
    "\n",
    "                # Compute gradients\n",
    "                # ===== Insert your code here =====\n",
    "\n",
    "                # Update parameters\n",
    "                # ===== Insert your code here =====\n",
    "\n",
    "            # Calculate and trace the loss\n",
    "            # ===== Insert your code here =====\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "        \"\"\"\n",
    "        Predict class probabilities for input samples.\n",
    "\n",
    "        Formula:\n",
    "        P(y=1|x) = σ(wx + b)\n",
    "        where σ is the sigmoid function\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples,)\n",
    "            Predicted probabilities\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "\n",
    "        \"\"\"\n",
    "        Predict class labels for input samples.\n",
    "\n",
    "        Formula:\n",
    "        y = 1 if P(y=1|x) >= threshold else 0\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "        threshold : float\n",
    "            Classification threshold\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples,)\n",
    "            Predicted class labels (0 or 1)\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vIH2qghmeak3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Confusion Matrix:\n",
      "[[ 63   2]\n",
      " [  2 133]]\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "\n",
    "# Generate random features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate binary target labels (0 or 1)\n",
    "# Let's create labels based on a linear combination of features plus some noise\n",
    "weights = np.random.randn(n_features)\n",
    "bias = np.random.randn(1)\n",
    "y_prob = 1 / (1 + np.exp(-(np.dot(X, weights) + bias)))  # Sigmoid to get probabilities\n",
    "y = (y_prob > 0.5).astype(int).flatten()  # Convert probabilities to binary labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Logistic Regression model\n",
    "model = LogisticRegression(learning_rate=0.01, n_epochs=100, batch_size=32)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression (Multinomial Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \"\"\"\n",
    "    Softmax Regression classifier using Stochastic Gradient Descent (SGD)\n",
    "    for multiclass classification problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the Softmax Regression model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            The step size for updating model parameters\n",
    "        n_epochs : int\n",
    "            Number of passes through the training data\n",
    "        batch_size : int\n",
    "            Number of training examples to use in each gradient update\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.W = None  # weights\n",
    "        self.b = None  # biases\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \"\"\"\n",
    "        Compute the softmax function.\n",
    "\n",
    "        Formula:\n",
    "        softmax(z) = exp(z) / Σ exp(z)\n",
    "        where z = Wx + b\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        z : array-like of shape (n_samples, n_classes)\n",
    "            Input values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            Softmax probabilities for each class\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def initialize_parameters(self, n_features, n_classes):\n",
    "        self.W = np.random.randn(n_features, n_classes) * np.sqrt(2 / n_features)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize model parameters using Xavier initialization.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_features : int\n",
    "            Number of input features\n",
    "        n_classes : int\n",
    "            Number of output classes\n",
    "        \"\"\"\n",
    "        # Xavier initialization for weights\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss for multiclass classification.\n",
    "\n",
    "        Formula:\n",
    "        L = -1/N * Σ Σ(y * log(ŷ))\n",
    "        where:\n",
    "        - y is true label\n",
    "        - ŷ is predicted probability\n",
    "        - N is number of samples\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like of shape (n_samples, n_classes)\n",
    "            True one-hot encoded labels\n",
    "        y_pred : array-like of shape (n_samples, n_classes)\n",
    "            Predicted probabilities\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average cross-entropy loss\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def compute_gradients(self, X_batch, y_batch, y_pred):\n",
    "        m = X_batch.shape[0]\n",
    "        dW = np.dot(X_batch.T, (y_pred - y_batch)) / m\n",
    "        db = np.sum(y_pred - y_batch, axis=0, keepdims=True) / m\n",
    "        return dW, db\n",
    "\n",
    "        \"\"\"\n",
    "        Compute gradients for weights and biases.\n",
    "\n",
    "        Formulas:\n",
    "        ∂L/∂W = 1/N * X^T * (ŷ - y)\n",
    "        ∂L/∂b = 1/N * Σ(ŷ - y)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_batch : array-like\n",
    "            Input features for current batch\n",
    "        y_batch : array-like\n",
    "            True labels for current batch\n",
    "        y_pred : array-like\n",
    "            Predicted probabilities for current batch\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Gradients for weights and biases\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the softmax regression model using mini-batch SGD.\n",
    "\n",
    "        Process:\n",
    "        1. Initialize parameters\n",
    "        2. For each epoch:\n",
    "            a. Shuffle data\n",
    "            b. Split into mini-batches\n",
    "            c. For each mini-batch:\n",
    "                - Compute forward pass (softmax)\n",
    "                - Compute gradients\n",
    "                - Update parameters\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values (class labels)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = y.shape[1]\n",
    "        self.initialize_parameters(n_features, n_classes)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Shuffle the data at the beginning of each epoch\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self.softmax(np.dot(X_batch, self.W) + self.b)\n",
    "\n",
    "                # Compute gradients\n",
    "                dW, db = self.compute_gradients(X_batch, y_batch, y_pred)\n",
    "\n",
    "                # Update parameters\n",
    "                self.W -= self.learning_rate * dW\n",
    "                self.b -= self.learning_rate * db\n",
    "\n",
    "            # Calculate and trace the loss for the entire dataset after each epoch\n",
    "            y_pred_full = self.predict_proba(X)\n",
    "            loss = self.compute_loss(y, y_pred_full)\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.softmax(np.dot(X, self.W) + self.b)\n",
    "\n",
    "        \"\"\"\n",
    "        Predict class probabilities for input samples.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            Predicted probabilities for each class\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "        \"\"\"\n",
    "        Predict class labels for input samples.\n",
    "\n",
    "        Formula:\n",
    "        y = argmax(P(y|x))\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples,)\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m     50\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m load_fashion_mnist()\n\u001b[0;32m---> 51\u001b[0m y_train_encoded \u001b[38;5;241m=\u001b[39m one_hot_encode(y_train)\n\u001b[1;32m     52\u001b[0m y_test_encoded \u001b[38;5;241m=\u001b[39m one_hot_encode(y_test)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Initialize and train the Softmax Regression model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 46\u001b[0m, in \u001b[0;36mone_hot_encode\u001b[0;34m(labels, num_classes)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot_encode\u001b[39m(labels, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(categories\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mrange\u001b[39m(num_classes)], sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoder\u001b[38;5;241m.\u001b[39mfit_transform(labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "import numpy as np\n",
    "import requests\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# URLs to download the Fashion MNIST dataset\n",
    "BASE_URL = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
    "FILENAMES = {\n",
    "    \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "    \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "    \"test_images\": \"t10k-images-idx3-ubyte.gz\",\n",
    "    \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",\n",
    "}\n",
    "\n",
    "# Download and extract Fashion MNIST dataset\n",
    "def download_and_load_mnist(filename, num_items, item_size, reshape_dims=None):\n",
    "    if not os.path.exists(filename):\n",
    "        urlretrieve(BASE_URL + filename.split(\"/\")[-1], filename)\n",
    "\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=item_size)\n",
    "        if reshape_dims:\n",
    "            return data.reshape(num_items, *reshape_dims)\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "# Load datasets\n",
    "def load_fashion_mnist():\n",
    "    train_images = download_and_load_mnist(\"train-images-idx3-ubyte.gz\", 60000, 16, (28, 28))\n",
    "    train_labels = download_and_load_mnist(\"train-labels-idx1-ubyte.gz\", 60000, 8)\n",
    "    test_images = download_and_load_mnist(\"t10k-images-idx3-ubyte.gz\", 10000, 16, (28, 28))\n",
    "    test_labels = download_and_load_mnist(\"t10k-labels-idx1-ubyte.gz\", 10000, 8)\n",
    "\n",
    "    # Reshape and normalize images\n",
    "    train_images = train_images.reshape(60000, 28*28) / 255.0\n",
    "    test_images = test_images.reshape(10000, 28*28) / 255.0\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "# One-hot encoding for labels\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    encoder = OneHotEncoder(categories=[range(num_classes)], sparse=False)\n",
    "    return encoder.fit_transform(labels.reshape(-1, 1))\n",
    "\n",
    "# Load data\n",
    "X_train, y_train, X_test, y_test = load_fashion_mnist()\n",
    "y_train_encoded = one_hot_encode(y_train)\n",
    "y_test_encoded = one_hot_encode(y_test)\n",
    "\n",
    "# Initialize and train the Softmax Regression model\n",
    "model = SoftmaxRegression(learning_rate=0.1, n_epochs=50, batch_size=64)\n",
    "model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
