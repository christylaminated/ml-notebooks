{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSX0E8XEdlE6"
   },
   "source": [
    "# Linear SVM (QP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cvxopt\n",
      "  Downloading cvxopt-1.3.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (1.3 kB)\n",
      "Downloading cvxopt-1.3.2-cp311-cp311-macosx_10_9_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cvxopt\n",
      "Successfully installed cvxopt-1.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pw94_bI7BRBS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "class LinearSVM_QP:\n",
    "    \"\"\"\n",
    "    Hard-margin Support Vector Machine classifier using Quadratic Programming (QP)\n",
    "    for binary classification problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the SVM model.\n",
    "        \"\"\"\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the SVM model using quadratic programming for hard-margin SVM.\n",
    "\n",
    "        Process:\n",
    "        --------\n",
    "        To train a hard-margin SVM, we maximize the margin between two classes\n",
    "        while ensuring that each point lies on the correct side of the hyperplane.\n",
    "        This can be framed as a quadratic programming (QP) problem.\n",
    "\n",
    "        We aim to solve:\n",
    "            minimize (1/2) * ||w||^2\n",
    "            subject to y_i * (w · x_i + b) >= 1, for all i\n",
    "\n",
    "        By introducing Lagrange multipliers (alphas), the problem can be reformulated:\n",
    "            maximize Σ α_i - (1/2) Σ Σ α_i * α_j * y_i * y_j * (x_i · x_j)\n",
    "            subject to Σ α_i * y_i = 0 and α_i >= 0, for all i\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values (should be -1 or 1)\n",
    "\n",
    "        QP Setup:\n",
    "        ----------\n",
    "        To solve this as a QP problem, we define:\n",
    "        - P: Coefficient matrix for the quadratic term, defined as (y * X) * (y * X)^T.\n",
    "        - q: Linear term in the objective, set to -1 for each sample.\n",
    "        - G and h: Inequality constraints to ensure λ_i ≥ 0 for each Lagrange multiplier λ.\n",
    "        - A and b: Equality constraint to enforce that the solution satisfies Σ λ_i * y_i = 0.\n",
    "\n",
    "        Notes:\n",
    "        ------\n",
    "        After solving the QP, the support vectors are determined by non-zero λ values.\n",
    "        The weight vector w and bias term b are then computed using these support vectors.\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Ensure labels are -1 and 1\n",
    "        y = y.astype(np.float64).reshape(-1,1) # make sure that y is a column vector\n",
    "\n",
    "        # Setup QP parameters\n",
    "        # P: coefficient matric for the quadratic term\n",
    "        P = matrix(np.dot(y * X, (y * X).T))\n",
    "        # q: Linear term in the objective, set to -1 for each sample\n",
    "        q = matrix(-np.ones((n_samples, 1)))\n",
    "        # G and h: inequality constaints to ensure lamba_i >=0\n",
    "        G = matrix(-np.eye(n_samples))\n",
    "        h = matrix(np.zeros((n_samples, 1)))\n",
    "        # A and b: equality constraint \n",
    "        A = matrix(y.T, (1, n_samples), 'd')\n",
    "        b = matrix(0.0)\n",
    "\n",
    "\n",
    "        # Solve QP problem\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Retrieve Lagrange multipliers\n",
    "        alphas = np.array(solution['x']).flatten()\n",
    "        support_vector_indicies = alphas > 1e-5\n",
    "        self.support_vectors_ = X[support_vector_indicies]\n",
    "        self.support_vector_labels_ = y[support_vector_indicies]\n",
    "        self.alphas_ = alphas[support_vector_indicies]\n",
    "\n",
    "        # Calculate weights (w) and bias (b)\n",
    "        self.w = np.sum(\n",
    "            self.alphas_[:, None] * self.support_vector_labels_ * self.support_vectors_, axis=0\n",
    "        )\n",
    "        self.b = np.mean(\n",
    "            self.support_vector_labels_ - np.dot(self.support_vectors_, self.w)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute decision function values for input samples.\n",
    "\n",
    "        Formula:\n",
    "        f(x) = wx + b\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples,)\n",
    "            Decision function values\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "        return np.dot(X, self.w) + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for input samples.\n",
    "\n",
    "        Formula:\n",
    "        y = sign(wx + b)\n",
    "        where sign(z) = 1 if z >= 0 else -1\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples,)\n",
    "            Predicted class labels (-1 or 1)\n",
    "        \"\"\"\n",
    "        # ===== Insert your code here =====\n",
    "        return np.sign(self.decision_function(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7yeHuSKE8Iz"
   },
   "source": [
    "# Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L9tA-yFoE-y-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Linear SVM with SGD\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LinearSVM_GD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Test Linear SVM with Stochastic Gradient Descent (SGD)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting Linear SVM with SGD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m linear_svm_gd \u001b[38;5;241m=\u001b[39m LinearSVM_GD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     49\u001b[0m linear_svm_gd\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     50\u001b[0m y_pred_linear \u001b[38;5;241m=\u001b[39m linear_svm_gd\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearSVM_GD' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a simple linearly separable dataset\n",
    "def generate_linear_separable_data():\n",
    "    # Class 1\n",
    "    X1 = np.random.randn(50, 2) + np.array([2, 2])\n",
    "    y1 = np.ones(50)\n",
    "\n",
    "    # Class -1\n",
    "    X2 = np.random.randn(50, 2) + np.array([-2, -2])\n",
    "    y2 = -np.ones(50)\n",
    "\n",
    "    # Combine the two classes\n",
    "    X = np.vstack((X1, X2))\n",
    "    y = np.hstack((y1, y2))\n",
    "    return X, y\n",
    "\n",
    "# Visualization function for SVM decision boundary\n",
    "def plot_decision_boundary(X, y, model, title=\"Decision Boundary\"):\n",
    "    # Create grid to evaluate model\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict on the grid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour and training points\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolors='k', marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "# Generate data\n",
    "X_train, y_train = generate_linear_separable_data()\n",
    "X_test, y_test = generate_linear_separable_data()\n",
    "\n",
    "# Test Linear SVM with Stochastic Gradient Descent (SGD)\n",
    "print(\"Testing Linear SVM with SGD\")\n",
    "linear_svm_gd = LinearSVM_GD(learning_rate=0.01, n_epochs=100, batch_size=16, C=1.0)\n",
    "linear_svm_gd.fit(X_train, y_train)\n",
    "y_pred_linear = linear_svm_gd.predict(X_test)\n",
    "accuracy_linear = np.mean(y_pred_linear == y_test)\n",
    "print(\"Linear SVM GD Test Accuracy:\", accuracy_linear)\n",
    "\n",
    "# Visualize Linear SVM SGD decision boundary\n",
    "plot_decision_boundary(X_train, y_train, linear_svm_gd, title=\"Linear SVM with SGD Decision Boundary\")\n",
    "\n",
    "# Test Hard-margin SVM with Quadratic Programming (QP)\n",
    "print(\"\\nTesting Linear SVM with QP\")\n",
    "linear_svm_qp = LinearSVM_QP()\n",
    "linear_svm_qp.fit(X_train, y_train)\n",
    "y_pred_hard_margin = linear_svm_qp.predict(X_test)\n",
    "accuracy_hard_margin = np.mean(y_pred_hard_margin == y_test)\n",
    "print(\"Linear SVM QP Test Accuracy:\", accuracy_hard_margin)\n",
    "\n",
    "# Visualize Hard-margin SVM QP decision boundary\n",
    "plot_decision_boundary(X_train, y_train, linear_svm_qp, title=\"Linear SVM with QP Decision Boundary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
